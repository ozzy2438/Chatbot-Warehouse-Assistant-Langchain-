# Amazon ETL Automation - Setup Guide

## ğŸ“‹ System Overview

Your ETL system is now clean and automated with Airflow:

**Pipeline Flow:**
```
1. Scrape â†’ 2. Transform â†’ 3. Load â†’ 4. Cleanup â†’ 5. Summary
```

## ğŸ—‚ï¸ Project Structure (Cleaned)

```
/Chatbot-Langchain-ProductFinder/
â”œâ”€â”€ airflow_dags/
â”‚   â”œâ”€â”€ amazon_etl_pipeline.py     â† NEW: Clean Airflow DAG
â”‚   â””â”€â”€ product_chatbot_pipeline.py (old - can remove)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Scraped CSVs
â”‚   â”œâ”€â”€ processed/                  # Split categories
â”‚   â””â”€â”€ archive/                    # Auto-archived (7+ days)
â”œâ”€â”€ scrape_bestsellers_2000.py     # Selenium scraper
â”œâ”€â”€ etl_orchestrator.py            # ETL transformer (all 6 steps)
â”œâ”€â”€ chatbot.py                     # LLM chatbot
â”œâ”€â”€ app_realtime.py                # Web UI
â””â”€â”€ README_ETL.md                  # Original docs
```

## ğŸš€ How to Use

### Option 1: Run Manually (Test)

```bash
# Test scraper
python scrape_bestsellers_2000.py

# Test full ETL pipeline
python etl_orchestrator.py --test
```

### Option 2: Run with Airflow (Production)

**Setup Airflow (if not already):**
```bash
# Install Airflow (already installed)
# pip install apache-airflow

# Initialize/migrate database
airflow db migrate
```

**Start Airflow (Standalone Mode - Airflow 3.x):**
```bash
# Run all-in-one Airflow (scheduler + webserver)
airflow standalone
```

**Access Airflow:**
1. Wait for startup (creates admin user automatically)
2. Check terminal output for username/password
3. Open: http://localhost:8080
4. Find DAG: `amazon_etl_pipeline`
5. Toggle ON to enable
6. Runs automatically daily at 2 AM

**Manual Trigger:**
Click "Trigger DAG" button to run immediately

## ğŸ“Š ETL Pipeline Tasks

### Task 1: Scrape Amazon
- Selenium scrapes 2000 bestsellers
- Output: `data/raw/raw_products_YYYYMMDD_HHMMSS.csv`

### Task 2: Transform Data
- Clean (remove nulls, convert types)
- Enrich (add inventory, customer metrics)
- Split into 4 categories:
  - Low stock alerts
  - High performers
  - Restock queue  
  - Active inventory

### Task 3: Load Vector DB
- Rebuilds ChromaDB vector store
- Creates embeddings for LLM chatbot

### Task 4: Cleanup
- Archives files older than 7 days
- Keeps `data/` directory clean

### Task 5: Summary
- Prints execution metrics
- Shows product counts

## ğŸ” Monitoring

**View Logs:**
```bash
# Airflow logs
ls ~/airflow/logs/dag_id=amazon_etl_pipeline/

# ETL orchestrator logs
tail -f etl_orchestrator.log
```

**Check Status:**
- Airflow UI: Task instance details
- XCom values show metrics between tasks

## âš™ï¸ Configuration

**Schedule:** Daily at 2 AM (change in DAG file)
**Retries:** 3 attempts with 5-minute delay
**Timeout:** 2 hours max execution

**To change schedule:**
Edit `airflow_dags/amazon_etl_pipeline.py`:
```python
schedule='0 2 * * *'  # Cron format
```

## ğŸ§¹ Files Removed

**Documentation (13 files):** AIRFLOW_SETUP.md, AUTOMATION.md, etc.
**Old scripts (7 files):** factory_etl_automation.py, monitoring_dashboard.py, etc.
**Old tests:** test_etl_pipeline.py, test_complex.py, test_voice_ui.py

## âœ… System Status

- âœ… Scraper: Working (`scrape_bestsellers_2000.py`)
- âœ… Transformer: All 6 steps in `etl_orchestrator.py`
- âœ… LLM: `chatbot.py` + `app_realtime.py`
- âœ… Airflow: New clean DAG created
- âœ… Project: Cleaned and organized

## ğŸ¯ Next Steps

1. Start Airflow (see setup above)
2. Enable the DAG in UI
3. Trigger manual run to test
4. Let it run daily automatically

EÄŸer sorun olursa, Airflow UI'dan task logs'una bakabilirsiniz!
