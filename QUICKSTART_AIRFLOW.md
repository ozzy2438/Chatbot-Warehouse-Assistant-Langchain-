# Quick Start: Airflow ETL Automation

## âš¡ Simple Setup (Airflow 3.x)

Your Airflow DAG is ready. Just run:

```bash
cd /Users/osmanorka/Chatbot-Langchain-ProductFinder

# Set DAGs folder
export AIRFLOW__CORE__DAGS_FOLDER=/Users/osmanorka/Chatbot-Langchain-ProductFinder/airflow_dags

# Start Airflow (all-in-one: webserver + scheduler + database)
airflow standalone
```

**That's it!** This single command:
- âœ… Initializes database
- âœ… Creates admin user (check terminal for password)
- âœ… Starts webserver on port 8080
- âœ… Starts scheduler
- âœ… Loads your DAG

## ğŸ“± Access Airflow UI

1. Wait ~30 seconds for startup
2. Look in terminal for:
   ```
   standalone | Airflow is ready
   standalone | Login with username: admin  password: <YOUR_PASSWORD>
   ```
3. Open: **http://localhost:8080**
4. Login with credentials from step 2
5. Find DAG: **amazon_etl_pipeline**
6. Toggle it ON (activate)

## ğŸ¯ What Happens

**Automatically:** Runs daily at 2 AM

**Manually:** Click "â–¶ Trigger DAG" button anytime

**Pipeline Steps:**
1. **Scrape** â†’ 2000 Amazon products
2. **Transform** â†’ Clean + enrich data
3. **Load** â†’ Update vector DB
4. **Cleanup** â†’ Archive old files
5. **Summary** â†’ Print metrics

## Alternative: Run ETL Manually (No Airflow)

If you don't want to use Airflow:

```bash
# Test mode (200 products)
python etl_orchestrator.py --test

# Full run (2000 products)
python etl_orchestrator.py
```

---

## ğŸ“ Notes

- **Airflow logs**: `~/airflow/logs/`
- **Database**: `~/airflow/airflow.db` (SQLite)
- **Stop**: Press `Ctrl+C` in terminal

TÃ¼mÃ¼ hazÄ±r! Sadece `airflow standalone` komutunu Ã§alÄ±ÅŸtÄ±r! ğŸš€
